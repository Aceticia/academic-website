---
title: "Mooney task: modeling human one-shot perceptual learning through disambiguation"
collection: publications
category: others
permalink: /publication/2022-06-01-Mooney-task
excerpt: 'We used a series of DNN models to model human one-shot learning, and found that they can show some level of human-like performance. By comparing their activations to human brain activations, we can find that some DNN models display representations similar to the ventral stream.'
date: 2022-06-01
venue: 'Others'
slidesurl: 'https://aceticia.github.io/academic-website/files/mooney_slides.pdf'
paperurl: 'https://aceticia.github.io/academic-website/files/MooneyBenchmarking_NeurIPS_2022_Preprint.pdf'
---


Abstract
======
Recent advances in deep neural networks (DNNs) suggest that they can function as powerful models of human and animal behavior and neural function. Thus, many studies have harnessed DNNs to model visual perception in the human brain in terms of task performance and internal representations. In this study, we propose the Mooney task, adapted from a one-shot perceptual learning task in humans. We develop a series of DNN models that apply temporal attention to the features generated by different backbone models that operate on individual images only. Our results are threefold. Firstly, our DNN models can replicate the behaviors observed in human volunteers without the need for task-specific backbone model finetuning. Secondly, we find ShuffleNet V2-based models to have the most brain-like representations overall. Thirdly, we find that representations in the temporal layers of our model, rather than those in the spatial layers, are more similar to low level and high level visual regions after one-shot learning. This suggests that recurrent neural mechanisms are involved in Mooney effect. Taken together, our study demonstrates the potential of using DNNâ€™s to model human perceptual learning, and improve our understanding of its mechanisms